{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1011047e-bb8b-4c1d-8c22-731b3ee20892",
   "metadata": {},
   "source": [
    "# Team 3 - Modeling - NLP with BERT Model\n",
    "\n",
    "Using pre-trained Bi-directional Encoder Representations with Transformers (BERT) Model, using base-uncased version. This model will use the Anime Synopsis data in order to generate a semantic representation of the synopsis which can be used as features for the KNN model. BERT outputs an embedding of each synopsis as a 768 length vector representation.\n",
    "\n",
    "\n",
    "#### Note!!!!\n",
    "**FYI, Sagemaker does not have pre-trained transformer models, and the 2 cpu instances in Learner Lab is not enough to run inferences for this large model. This was run on a local computer with a Nvidia RTX A6000 GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da429a4-0ee9-441a-8214-1743d86bd774",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85e206b6-8f6b-4597-8c4e-f3693af75b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "def get_semantics(text):\n",
    "    \n",
    "    # Add the special tokens.\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "    # Split the sentence into tokens.\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "    # Map the token strings to their vocabulary indeces.\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "        hidden_states = outputs[2]\n",
    "\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "    token_embeddings.size()\n",
    "\n",
    "    # remove batch size\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "    # The first number = length of sentence. # words\n",
    "    token_embeddings.size()\n",
    "\n",
    "    # Stores the token vectors, with shape [22 x 768]\n",
    "    token_vecs_sum = []\n",
    "\n",
    "    # `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "\n",
    "        # `token` is a [12 x 768] tensor\n",
    "\n",
    "        # Sum the vectors from the last four layers.\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "\n",
    "        # Use `sum_vec` to represent `token`.\n",
    "        token_vecs_sum.append(sum_vec)\n",
    "\n",
    "    # `hidden_states` has shape [13 x 1 x 22 x 768]\n",
    "\n",
    "    # `token_vecs` is a tensor with shape [22 x 768]\n",
    "    token_vecs = hidden_states[-2][0]\n",
    "\n",
    "    # Calculate the average of all 22 token vectors.\n",
    "    sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "    sentence_embedding.shape\n",
    "    \n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1df5bece-9fe7-4e59-8019-a8a1a908aba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_synopsis = pd.read_csv('anime_with_synopsis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35cbc0ba-dc64-425a-b701-3e4f2fbee850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any non-alphanumeric characters other than basic punctuations\n",
    "anime_synopsis['sypnopsis'] = anime_synopsis['sypnopsis'].fillna('')\n",
    "anime_synopsis['sypnopsis'] = anime_synopsis['sypnopsis'].str.replace(\"[^a-zA-Z0-9 .,']\", '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc7f08f1-8f8a-4089-b5c9-def67b9e969b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16214/16214\r"
     ]
    }
   ],
   "source": [
    "# The Bert model's output shape is 768 columns\n",
    "for i, x in anime_synopsis.iterrows():\n",
    "    \n",
    "    if i < 11451:\n",
    "        continue\n",
    "        \n",
    "    synopsis = x['sypnopsis']\n",
    "\n",
    "    if synopsis == '' or pd.isna(x['sypnopsis']):\n",
    "        semantic = [0]*768\n",
    "    else:\n",
    "        # can take upto 512 characters\n",
    "        semantic = get_semantics(x['sypnopsis'][:512])\n",
    "\n",
    "    semantic = pd.DataFrame(get_semantics(x['sypnopsis'][:512])).transpose()\n",
    "\n",
    "    if i==0:\n",
    "        semantic.to_csv('semantics.csv', index=False)\n",
    "    else:\n",
    "        semantic.to_csv('semantics.csv', index=False, mode='a', header=None)\n",
    "\n",
    "    print('{}/{}'.format(i+1, df.shape[0]), flush=True, end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be9969d0-3029-459f-b8b0-cc59aea5f209",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantics = pd.read_csv('semantics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3af318bd-bef4-4dd9-9c0e-d2ef23d170eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16214, 768), (16214, 5))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantics.shape, df.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
